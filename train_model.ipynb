{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, pickle, sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy import misc\n",
    "from glob import glob\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from utils import rescale, weight_variable, bias_variable, conv2d, max_pool, image_path, tmp_image_file, model_file, max_shape\n",
    "\n",
    "# PARAMETERS\n",
    "n_epochs = 100\n",
    "\n",
    "# CNN parameters\n",
    "n_convo_layer1 = 32\n",
    "n_convo_layer2 = 32\n",
    "inner_layer_size = 50\n",
    "percep_size = 1024\n",
    "\n",
    "# training: batch size\n",
    "batch_len = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - Import images, rotate and rescale them\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.exists(tmp_image_file):\n",
    "    images = pickle.load(open(tmp_image_file, \"rb\"))\n",
    "    \n",
    "else:\n",
    "    images = dict()\n",
    "    for what in ['sushi', 'sandwich']:\n",
    "        print(\"Loading images for '%s'...\" % what)\n",
    "        files = glob(image_dir % what)\n",
    "        full_images = [misc.imread(f, mode=\"L\") for f in files]\n",
    "        images[what] = [rescale(img, max_shape) for img in full_images]\n",
    "        \n",
    "        for angle in [90, 180, 270]:\n",
    "            print(\"-- rotating by %d degrees...\" % angle)\n",
    "            images[what].extend([rescale(misc.imrotate(img, angle), max_shape) for img in full_images])\n",
    "    \n",
    "    pickle.dump(images, open(tmp_image_file, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 - define the Tensorflow graph\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "# input layer\n",
    "X = tf.placeholder(tf.float32, shape=[None, max_shape[0], max_shape[1], 1])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# first convolution\n",
    "W_conv1 = weight_variable([10, 10, 1, n_convo_layer1])\n",
    "b_conv1 = bias_variable([n_convo_layer1])\n",
    "\n",
    "h_conv1 = tf.nn.sigmoid(conv2d(X, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool(h_conv1, ksize=[1, 2, 2, 1])\n",
    "\n",
    "# second convolution\n",
    "W_conv2 = weight_variable([10, 10, n_convo_layer1, n_convo_layer2])\n",
    "b_conv2 = bias_variable([n_convo_layer2])\n",
    "\n",
    "h_conv2 = tf.nn.sigmoid(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool(h_conv2, ksize=[1, 2, 2, 1])\n",
    "\n",
    "# first fully connected layer\n",
    "W_fc1 = weight_variable([inner_layer_size * inner_layer_size * n_convo_layer2, percep_size])\n",
    "b_fc1 = bias_variable([percep_size])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, inner_layer_size * inner_layer_size * n_convo_layer2])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# second fully connected layer\n",
    "W_fc2 = weight_variable([percep_size, percep_size])\n",
    "b_fc2 = bias_variable([percep_size])\n",
    "\n",
    "h_fc2 = tf.nn.tanh(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "# output layer\n",
    "W_fc3 = weight_variable([percep_size, 1])\n",
    "b_fc3 = bias_variable([1])\n",
    "\n",
    "logits = tf.matmul(h_fc2_drop, W_fc3) + b_fc3\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "train_step = tf.train.RMSPropOptimizer(1e-5).minimize(cross_entropy)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 - Set aside some samples for testing\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_array = np.stack(images['sushi'] + images['sandwich']).reshape(-1, max_shape[0], max_shape[1], 1)\n",
    "y_array = np.array([1] * len(images['sushi']) + [0] * len(images['sandwich'])).reshape(-1, 1)\n",
    "\n",
    "test_ix = np.random.choice(len(X_array), batch_len)\n",
    "train_ix = np.setdiff1d(range(len(X_array)), test_ix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 - Train the CNN\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; cross-entropy: 0.800\n",
      "AUC: train 0.48, test 0.52\n",
      "epoch 1; cross-entropy: 0.819\n",
      "AUC: train 0.57, test 0.59\n",
      "epoch 2; cross-entropy: 0.799\n",
      "AUC: train 0.56, test 0.67\n",
      "epoch 3; cross-entropy: 0.656\n",
      "AUC: train 0.68, test 0.64\n",
      "epoch 4; cross-entropy: 0.840\n",
      "AUC: train 0.55, test 0.59\n",
      "epoch 5; cross-entropy: 0.682\n",
      "AUC: train 0.64, test 0.72\n",
      "epoch 6; cross-entropy: 0.706\n",
      "AUC: train 0.59, test 0.69\n",
      "epoch 7; cross-entropy: 0.607\n",
      "AUC: train 0.78, test 0.82\n",
      "epoch 8; cross-entropy: 0.770\n",
      "AUC: train 0.63, test 0.67\n",
      "epoch 9; cross-entropy: 0.701\n",
      "AUC: train 0.55, test 0.68\n",
      "epoch 10; cross-entropy: 0.708\n",
      "AUC: train 0.54, test 0.76\n",
      "epoch 11; cross-entropy: 0.774\n",
      "AUC: train 0.48, test 0.73\n",
      "epoch 12; cross-entropy: 0.735\n",
      "AUC: train 0.61, test 0.81\n",
      "epoch 13; cross-entropy: 0.728\n",
      "AUC: train 0.44, test 0.75\n",
      "epoch 14; cross-entropy: 0.602\n",
      "AUC: train 0.73, test 0.77\n",
      "epoch 15; cross-entropy: 0.703\n",
      "AUC: train 0.52, test 0.71\n",
      "epoch 16; cross-entropy: 0.677\n",
      "AUC: train 0.63, test 0.72\n",
      "epoch 17; cross-entropy: 0.640\n",
      "AUC: train 0.69, test 0.68\n",
      "epoch 18; cross-entropy: 0.739\n",
      "AUC: train 0.55, test 0.72\n",
      "epoch 19; cross-entropy: 0.620\n",
      "AUC: train 0.71, test 0.73\n",
      "epoch 20; cross-entropy: 0.666\n",
      "AUC: train 0.57, test 0.76\n",
      "epoch 21; cross-entropy: 0.750\n",
      "AUC: train 0.59, test 0.69\n",
      "epoch 22; cross-entropy: 0.686\n",
      "AUC: train 0.61, test 0.70\n",
      "epoch 23; cross-entropy: 0.578\n",
      "AUC: train 0.86, test 0.75\n",
      "epoch 24; cross-entropy: 0.714\n",
      "AUC: train 0.56, test 0.72\n",
      "epoch 25; cross-entropy: 0.614\n",
      "AUC: train 0.73, test 0.71\n",
      "epoch 26; cross-entropy: 0.806\n",
      "AUC: train 0.45, test 0.80\n",
      "epoch 27; cross-entropy: 0.694\n",
      "AUC: train 0.59, test 0.72\n",
      "epoch 28; cross-entropy: 0.684\n",
      "AUC: train 0.64, test 0.78\n",
      "epoch 29; cross-entropy: 0.725\n",
      "AUC: train 0.56, test 0.81\n",
      "epoch 30; cross-entropy: 0.688\n",
      "AUC: train 0.70, test 0.71\n",
      "epoch 31; cross-entropy: 0.779\n",
      "AUC: train 0.57, test 0.81\n",
      "epoch 32; cross-entropy: 0.607\n",
      "AUC: train 0.73, test 0.81\n",
      "epoch 33; cross-entropy: 0.668\n",
      "AUC: train 0.63, test 0.81\n",
      "epoch 34; cross-entropy: 0.792\n",
      "AUC: train 0.55, test 0.81\n",
      "epoch 35; cross-entropy: 0.721\n",
      "AUC: train 0.61, test 0.81\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-23e2333df9d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_ix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_ix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_ix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bean/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bean/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bean/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/bean/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bean/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    np.random.shuffle(train_ix)\n",
    "    for batch_ix in np.array_split(train_ix, len(train_ix) / batch_len):\n",
    "        sess.run(train_step, feed_dict={X: X_array[batch_ix], y: y_array[batch_ix], keep_prob: 0.9})\n",
    "    \n",
    "    train_sample = np.random.choice(train_ix, batch_len)\n",
    "    ce, train_logits = sess.run([cross_entropy, logits], feed_dict={X: X_array[train_sample], y: y_array[train_sample], keep_prob: 1.0})\n",
    "    train_auc = roc_auc_score(y_array[train_sample], train_logits)\n",
    "    print(\"epoch {0:,}; cross-entropy: {1:.3f}\".format(epoch, ce))\n",
    "    \n",
    "    test_logits = sess.run(logits, feed_dict={X: X_array[test_ix], y: y_array[test_ix], keep_prob: 1})\n",
    "    test_auc = roc_auc_score(y_array[test_ix], test_logits)\n",
    "    \n",
    "    print(\"AUC: train {0:.2f}, test {1:.2f}\".format(train_auc, test_auc))\n",
    "\n",
    "[tf.add_to_collection('test', x) for x in [X, y, keep_prob, logits]]\n",
    "saver.save(sess, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
